{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a55cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "contest_folder = 'contestData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ade329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import get_current_status, process_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b784cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped, aggregated_df = get_current_status(contest_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd7d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_list = aggregated_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d4f1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e9cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not outputs found yet\n",
      "Not outputs found yet\n",
      "Not outputs found yet\n",
      "Running analysis at 0024\n",
      "Running analysis at 0024 for Wildcard Submissions\n",
      "Running analysis at 0054\n",
      "Running analysis at 0054 for Prime Subtractorization\n",
      "Running analysis at 0054 for Subsonic Subway\n",
      "Running analysis at 0054 for Substantial Losses\n",
      "Running analysis at 0054 for Substitution Cipher\n",
      "Running analysis at 0054 for Wildcard Submissions\n",
      "Running analysis at 0124\n",
      "Running analysis at 0124 for Subsonic Subway\n",
      "Running analysis at 0124 for Substantial Losses\n",
      "Running analysis at 0124 for Substitution Cipher\n",
      "Running analysis at 0124 for Wildcard Submissions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/htong/Desktop/hackercup/analysis.py\", line 274, in process_row\n",
      "    openai_judgment = call_openai(judgment_instructions)\n",
      "  File \"/Users/htong/Desktop/hackercup/analysis.py\", line 213, in call_openai\n",
      "    completion = client.chat.completions.create(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_utils/_utils.py\", line 272, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/resources/chat/completions.py\", line 645, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 1088, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 853, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 930, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length has been exceeded. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis at 0154\n",
      "Running analysis at 0154 for Prime Subtractorization\n",
      "Running analysis at 0154 for Subsonic Subway\n",
      "Running analysis at 0154 for Substitution Cipher\n",
      "Running analysis at 0154 for Wildcard Submissions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/htong/Desktop/hackercup/analysis.py\", line 274, in process_row\n",
      "    openai_judgment = call_openai(judgment_instructions)\n",
      "  File \"/Users/htong/Desktop/hackercup/analysis.py\", line 213, in call_openai\n",
      "    completion = client.chat.completions.create(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_utils/_utils.py\", line 272, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/resources/chat/completions.py\", line 645, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 1088, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 853, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 930, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length has been exceeded. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis at 0224\n",
      "Running analysis at 0224 for Prime Subtractorization\n",
      "Running analysis at 0224 for Subsonic Subway\n",
      "Running analysis at 0224 for Substantial Losses\n",
      "Running analysis at 0224 for Wildcard Submissions\n",
      "Running analysis at 0254\n",
      "Running analysis at 0254 for Prime Subtractorization\n",
      "Running analysis at 0254 for Substantial Losses\n",
      "Running analysis at 0324\n",
      "Running analysis at 0324 for Prime Subtractorization\n",
      "Running analysis at 0324 for Substantial Losses\n",
      "Running analysis at 0355\n",
      "Running analysis at 0355 for Prime Subtractorization\n",
      "Running analysis at 0355 for Substantial Losses\n",
      "Running analysis at 0425\n",
      "Running analysis at 0425 for Substitution Cipher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/htong/Desktop/hackercup/analysis.py\", line 274, in process_row\n",
      "    openai_judgment = call_openai(judgment_instructions)\n",
      "  File \"/Users/htong/Desktop/hackercup/analysis.py\", line 213, in call_openai\n",
      "    completion = client.chat.completions.create(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_utils/_utils.py\", line 272, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/resources/chat/completions.py\", line 645, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 1088, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 853, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 930, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length has been exceeded. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis at 0455\n",
      "Running analysis at 0455 for Substitution Cipher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-30:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/htong/Desktop/hackercup/analysis.py\", line 274, in process_row\n",
      "    openai_judgment = call_openai(judgment_instructions)\n",
      "  File \"/Users/htong/Desktop/hackercup/analysis.py\", line 213, in call_openai\n",
      "    completion = client.chat.completions.create(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_utils/_utils.py\", line 272, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/resources/chat/completions.py\", line 645, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 1088, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 853, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 930, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length has been exceeded. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis at 0525\n",
      "Running analysis at 0525 for Substitution Cipher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-31:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/htong/Desktop/hackercup/analysis.py\", line 274, in process_row\n",
      "    openai_judgment = call_openai(judgment_instructions)\n",
      "  File \"/Users/htong/Desktop/hackercup/analysis.py\", line 213, in call_openai\n",
      "    completion = client.chat.completions.create(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_utils/_utils.py\", line 272, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/resources/chat/completions.py\", line 645, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 1088, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 853, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/openai/_base_client.py\", line 930, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length has been exceeded. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis at 0555\n",
      "Running analysis at 0555 for Substitution Cipher\n",
      "Running analysis at 0625\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "while True:\n",
    "    _, aggregated_df = get_current_status(contest_folder)\n",
    "    if len(aggregated_df) == 0:\n",
    "        print(\"Not outputs found yet\")\n",
    "        time.sleep(30)\n",
    "        continue\n",
    "    \n",
    "    aggregated_list = aggregated_df.to_dict(orient='records')\n",
    "    print(f\"Running analysis at {aggregated_df['timestring'][0]}\")\n",
    "    \n",
    "    for row in aggregated_list:\n",
    "        if row[\"status\"] != \"not_analyzed\":\n",
    "            continue\n",
    "        analysis_thread = threading.Thread(target=process_row, args=(row,))\n",
    "        analysis_thread.start()\n",
    "        print(f\"Running analysis at {row['timestring']} for {row['problem_name']}\")\n",
    "\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69715e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beec3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eff747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
